Exploring the SVM-KNN with Vision Problems

Joon Hyuck Choi (jchoilOO, jchoi100@jhu.edu)
Joo Chang Lee (jlee381, jlee381@jhu.edu)

November 20, 2016

1 Abstract

The k-Nearest Neighbor (KNN) algorithm is a non-parametric method used for classiﬁcation and
regression. In a previous homework assignment, we implemented the standard KNN classifier
and one of its variants, the distance weighted KNN. We felt the need to take another step from
this assignment and explore another type of the KNN algorithm, which is a bit more involved.

The KNN variant that we explore in this work is the SVM-KNN, a KNN algorithm that makes
use of a kernel multiclass SVM as a subprocedure. The nearest neighbor approach in visual
recognition problems has proven to work well in the past [1]. However, despite its beneﬁts, be»
cause the NN approach may suffer from high variation due to ﬁnite sampling. The incorporation
of an SVM can remedy this situation.

Note that the two extreme cases of the SVM-KNN are the standard KNN for small K values
and the regular SV‘M for K = n: The algorithm ﬁrst attempts multiclass classiﬁcation using the
standard KNN with a unanimous voting scheme. When at least one vote is different, it turns to
the kernel mutliclass SVM for prediction. '

Using a SVM can be effective in the neighborhood of a small number of examples and a small
number of classes [1]. We aim towards testing this algorithm on two datasets: MNIST [3] and
USPS [4]. Extended goals include testing the algorithm on the Caltech 101 [5] dataset, which
consists of more sophisticated real world images with more labels.

2 Methods

First run our own standard KNN with unanimous voting. In case of disagreement, use the kernel
SVM from scikitlearn with our own distance metric as input. Implement on our own the kernel
trick provided in [1] as necessary:

1 1
K(x,y) =< 2:,y >= §(< 1:1: > + <i,y > — < :c —y,:c——y >) = 5(d($,0) +d(y,0) —d(.r,y)) 